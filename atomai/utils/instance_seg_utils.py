import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage import measure
import pandas as pd
from numpy import array_equal
import matplotlib.patches as mpatches
from typing import Tuple
import matplotlib.colors as mcolors

def RGBtoGRAY(img: np.ndarray) -> np.ndarray:
  """
  Converts an RGB image to grayscale.

  Args:
      img (np.ndarray): The input RGB image. It is assumed to have a shape 
                         of (height, width, 3 channels).

  Returns:
      np.ndarray: The grayscale image. It will have the same size as the input 
                   image but with only one channel.
  """

  # Extract the red channel and return it as the grayscale image
  gray_img = img[:, :, 0]
  return gray_img


def make_borders(img: np.ndarray, border_size: int = 3, erode=True, dilate=True) -> np.ndarray:
  """
  Adds a border of specified size to a binary image.

  Args:
      img (np.ndarray): The binary image (expected to have only 0 and 1 pixel values).
      border_size (int, optional): The thickness of the border in pixels. Defaults to 3.
      erode (bool, optional): Whether to erode the image before dilation (removes pixels at edges). Defaults to True.
      dilate (bool, optional): Whether to dilate the image after erosion (expands the object). Defaults to True.

  Returns:
      np.ndarray: The image with a border added.
                  - 0: Background
                  - 1: Foreground (unchanged)
                  - 2: Border region
  """

  # Perform erosion if enabled (removes pixels at the edges)
  if erode:
    erosion_kernel = np.ones((3, 3))  # Square kernel for even erosion
    eroded_image = cv2.erode(img, erosion_kernel, iterations=1)
  else:
    eroded_image = img.copy()  # Copy the image if erosion is disabled

  # Create dilation kernel based on border size
  kernel_size = 2 * border_size + 1
  dilation_kernel = np.ones((kernel_size, kernel_size))  # Square kernel for uniform dilation

  # Perform dilation if enabled (expands the object)
  if dilate:
    dilated = cv2.dilate(eroded_image, dilation_kernel, iterations=1)
  else:
    dilated = eroded_image.copy()  # Copy the eroded image if dilation is disabled

  # Invert the dilated image and scale by 2 to create border region (values between 1 and 2)
  border = (1 - dilated) * 2

  # Combine eroded image (foreground), original image (unchanged background), and border
  original_with_border = np.where(eroded_image > 0, 1, border)

  return original_with_border



def watershed_segmentation(img_base: np.ndarray, img_mask: np.ndarray, border_size: int = 3) -> np.ndarray:
  """
  Performs watershed segmentation on the base image using the provided mask.

  Args:
      img_base (np.ndarray): The base image, expected to be grayscale (single channel) or RGB (3 channels).
      img_mask (np.ndarray): The mask image with the same shape as the base image. 
                             - 0: Uncertain region (used for segmentation)
                             - 1: Foreground (particle)
      border_size (int, optional): The size of the border region around the foreground. Defaults to 3.

  Returns:
      np.ndarray: The segmentation labels for the image. 
                  - 0: Border region
                  - 1: Foreground (particle)
                  - Background: Represented by the absence of a label (remains 0 from the mask)
  """

  # Create markers based on the mask
  sure_foreground = np.where(img_mask == 1, 1, 0).astype(np.uint8)
  # Markers are used to identify regions during watershed segmentation.

  # Find connected components in the mask (foreground)
  ret3, markers = cv2.connectedComponents(sure_foreground)
  # ret3 is not used here but can be informative about the number of foreground components found.

  # Ensure background has a distinct label (different from 0)
  markers += 10  # Add a constant value to separate background from foreground markers

  # Set uncertain regions (mask value of 0) back to 0 in the markers
  markers = np.where(img_mask == 0, 0, markers)
  # Watershed requires clear separation between foreground and background/uncertain regions.

  # Convert to RGB necessary for watershed algorithm
  if img_base.shape[-1] != 3:
    img_base = cv2.cvtColor(img_base, cv2.COLOR_GRAY2BGR)
  # Watershed in OpenCV expects a 3-channel (RGB) image.

  # Perform watershed segmentation on the image using the markers
  markers = cv2.watershed(img_base, markers)
  return markers



def plot_instantiated(markers: np.ndarray, force_borders=True):
  """
  Plots instantiated regions with different colors and black borders.

  Args:
    markers (np.ndarray): The markers image generated by watershed segmentation.
      This image should be a grayscale image where each pixel intensity represents
      a specific region.
    force_borders (bool, optional): If True, draws black borders around all regions
      even if there are no explicit boundaries present in the `markers` image. Defaults to True.
  Returns:
      None. This function displays the colored and bordered image using matplotlib.pyplot.
  """

  # Generate random colors for each unique marker (excluding background)
  unique_markers = np.unique(markers)
  colors = np.random.randint(0, 255, size=(len(unique_markers) - 1, 3))

  # Create an empty output image with the same dimensions and data type as the converted markers image (converted to BGR format)
  output_image = np.zeros_like(cv2.cvtColor(markers.astype(np.uint8), cv2.COLOR_GRAY2BGR))

  # Color the segments and highlight boundaries
  for marker_idx in range(len(unique_markers)):
    marker = unique_markers[marker_idx]

    if marker == -1:  # Boundary
      # Highlight the boundary in black
      color = [0, 0, 0]
    elif marker == 10:  # Special case (you can modify this logic)
      color = [152, 252, 152]  # Assign a specific green color
    else:
      color = colors[marker_idx - 2]  # Assign random color from the pool

    # Create a mask to identify pixels belonging to the current marker
    mask = markers == marker

    # Color the pixels in the output image based on the mask and chosen color
    output_image[mask] = color

  # Draw borders if necessary (no explicit boundaries or force_borders is True)
  if -1 not in unique_markers or force_borders:
    # Find the contours of the current segment mask
    contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    # Draw black contours (borders) on the output image with a thickness of 3
    cv2.drawContours(output_image, contours, -1, (0, 0, 0), thickness=3)

  # Display the final colored and bordered image
  plt.imshow(output_image)




def region_properties(img_base: np.ndarray, markers: np.ndarray):
  """
  Calculates properties of regions in an image using markers from watershed segmentation.

  Args:
    img_base (np.ndarray): The base image.
    markers (np.ndarray): The markers image generated by watershed segmentation. 
      This image should have the same dimensions as `img_base` where each pixel intensity 
      represents a specific region.

  Returns:
    pandas.DataFrame: A DataFrame containing the calculated properties for each region.
  """

  # Calculate properties for each region using skimage.measure.regionprops_table
  props = measure.regionprops_table(markers, intensity_image=img_base, 
                                  properties=['area', 'perimeter', 'equivalent_diameter','major_axis_length',
                                              'minor_axis_length','eccentricity', 'orientation',  'solidity'])

  # Convert properties dictionary to a pandas DataFrame
  df = pd.DataFrame(props)

  # Return DataFrame excluding potentially irrelevant first row
  return df[1:]




def marker_to_polygon(markers: np.ndarray, eps: float = 0.01, visualize: bool = True):
  """
  Converts regions in a marker image (generated by watershed) to polygon approximations.

  Args:
    markers (np.ndarray): The markers image containing integer values representing regions.
    eps (float, optional): Epsilon value controlling the approximation accuracy of polygons.
      Smaller values result in more precise (and potentially more complex) polygons, but must be greater than zero. Defaults to 0.01.
    visualize (bool, optional): If True, displays the image with the approximated polygons overlaid. Defaults to True.

  Returns:
    list: A list of NumPy arrays representing the approximated polygons. Each polygon is an array of coordinates. 
  """

  # Initialize an empty list to store the approximated polygons
  polygons = []

  # Iterate through unique marker values (excluding background and boundary)
  for marker in np.unique(markers):
    if marker == -1 or marker == 10:  # Skip background and watershed boundary
      continue

    # Create a binary mask for the current marker
    mask = np.zeros(markers.shape, dtype=np.uint8)
    mask[markers == marker] = 255

    # Find contours for the current marker region
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Approximate each contour to a polygon and add it to the list
    for contour in contours:
      # Calculate epsilon based on contour arclength for better precision 
      epsilon = eps * cv2.arcLength(contour, True)
      # Apply polygonal approximation with epsilon tolerance
      polygon = cv2.approxPolyDP(contour, epsilon, True)
      polygons.append(polygon)

  # Optional visualization of the result with polygons overlaid on the marker image
  if visualize:
    output_image = cv2.cvtColor(markers.astype(np.uint8), cv2.COLOR_GRAY2BGR)
    for polygon in polygons:
      cv2.polylines(output_image, [polygon], True, (0, 255, 0), 2)  # Draw green polygons
    plt.imshow(output_image)

  # Return the list of approximated polygons
  return polygons




def convert_preds_to_classes(preds: np.ndarray):
  """
  Converts prediction probabilities to class labels.

  Args:
    preds (np.ndarray): A NumPy array containing prediction probabilities.
      - If the last dimension has size 1 or the array has only 2 dimensions (one-hot encoded), 
        it's assumed to be a single class probability. Thresholding at 0.5 is used for conversion.
      - Otherwise, it's assumed to be multi-class probabilities. The class with the highest probability 
        is chosen using argmax.

  Returns:
    np.ndarray: A NumPy array of the same shape as the input `preds` but containing integer class labels.
  """

  # Handle single-class predictions (one-hot encoded or single probability)
  if preds.shape[-1] == 1 or len(preds.shape) == 2:
    # Thresholding at 0.5 to convert probabilities to classes (0 or 1)
    classes = np.where(preds >= 0.5, 1, 0)
    # Extract the first channel if one-hot encoded (shape will be (..., 1)) 
    classes = classes[:, :, 0]
    # Convert to uint8 for consistency (assuming class labels are integers)
    return classes.astype(np.uint8)

  # Handle multi-class predictions
  else:
    # Find the index of the class with the maximum probability for each sample (argmax)
    classes = np.argmax(preds, axis=-1)
    # Convert to uint8 for consistency (assuming class labels are integers)
    return classes.astype(np.uint8)


def pred_to_prop_masks(images_base: np.ndarray, preds: np.ndarray, is_bordered=False):
  """
  Performs watershed segmentation on images based on predictions and generates properties and polygons for each region.

  Args:
    images_base (np.ndarray): A NumPy array containing the base grayscale images.
    preds (np.ndarray): A NumPy array containing the predictions from the AtomAI Segmentor model.
      These predictions are likely probability maps where higher values indicate higher confidence 
      for a specific foreground object.
    is_bordered (bool, optional): If True, assumes the predictions already include separate channels 
      for boundaries/uncertain regions. If False (default), creates borders around the predicted segments.

  Returns:
    tuple: A tuple containing three elements:
      - markers_set (list): List of NumPy arrays, each representing the final watershed markers for an image.
      - properties_set (list): List of Pandas DataFrames, each containing geometric properties for regions in an image.
      - polygon_set (list): List of NumPy arrays, each representing the approximated polygons for regions in an image.
  """

  markers_set = []  # List to store watershed markers for each image
  properties_set = []  # List to store DataFrames with region properties for each image
  polygon_set = []  # List to store approximated polygons for each image

  for pred, base in zip(preds, images_base):
    # Create borders around predicted segments if not already provided
    if not is_bordered:
      bordered_img = make_borders(pred, erode=False, dilate=False)  # Function to create borders (implementation not provided)

    # Perform watershed segmentation using the base image and borders (if applicable)
    markers = watershed_segmentation(base, bordered_img)  # Function to perform segmentation (implementation not provided)
    # Optional: Display the segmentation result (may be commented out for efficiency)
    # plt.show()

    # Add the watershed markers for this image to the list
    markers_set.append(markers)

    # Calculate geometric properties of regions in the image using the markers and base image
    df = region_properties(base, markers)  # Function to calculate properties (provided earlier)
    # Add the DataFrame with properties to the list
    properties_set.append(region_properties(base,markers))

    # Generate polygon approximations for regions in the image using the markers (without visualization)
    polygon_set.append(marker_to_polygon(markers, visualize=False))  # Function to create polygons (provided earlier)

  # Return the lists of markers, properties DataFrames, and polygon approximations
  return markers_set, properties_set, polygon_set

def poly_to_coco(polygons_all: list, img_annots: list, category_id=1, start_test_id=1):
  """
  Converts a list of polygons to COCO annotation format.

  Args:
    polygons_all (list): List of lists. The outer list represents polygons for each image,
      and the inner lists contain polygon coordinates for each object in that image. Each polygon
      is a NumPy array.
    img_annots (list): List of dictionaries containing image annotations, likely in a format 
      expected by the COCO API (e.g., image ID, filename).
    category_id (int, optional): ID for the category of the objects represented by the polygons. Defaults to 1.
    start_test_id (int, optional): Starting ID for the image annotations. Defaults to 1.

  Returns:
    dict: A dictionary representing the COCO annotation for the provided polygons and image annotations.
  """

  # COCO annotation structure with some default values
  coco_annotation = {
      "licenses": [{"name": "", "id": 0, "url": ""}],  # License information (not used here)
      "info": {"contributor": "", "date_created": "", "description": "", "url": "", "version": "", "year": ""},  # Dataset information (not used here)
      "categories": [
          {
              "id": category_id,
              "name": "porosity"  # Assuming polygons represent "porosity" objects
          }
      ],
      "images": img_annots,  # List of image annotations provided
      "annotations": []  # List to store COCO annotations for each polygon
  }
  '''
    Polygons_all
    |_ Polygons of each Image
        |_ Polygons for each object
    
  '''

  # Iterate through each image's polygons
  idx = 0  # Annotation ID counter
  for test_idx, polygons_of_image in enumerate(polygons_all):
    for polygon in polygons_of_image:
      # Flatten polygon points into a single list
      segmentation = polygon.flatten().tolist()

      # Skip polygons with less than 6 points (might be invalid)
      if len(segmentation) < 6:
        continue

      # Create a bounding box from the polygon using OpenCV
      x, y, w, h = cv2.boundingRect(polygon)

      # Create a COCO annotation dictionary for this polygon
      annotation = {
          "id": idx + 1,  # Unique annotation ID
          "image_id": test_idx + start_test_id,  # Image ID based on index and starting ID
          "category_id": category_id,  # Category ID (all polygons share the same category here)
          "segmentation": [segmentation],  # List of polygon points (flattened)
          "area": w * h,  # Area of the bounding box
          "bbox": [x, y, w, h],  # Bounding box coordinates
          "iscrowd": 1,  # Mark as crowd instance (might not be necessary depending on the data)
          "attributes": {'occluded': False}  # Additional attribute (can be extended for more properties)
      }

      # Add the annotation to the COCO annotation dictionary
      coco_annotation["annotations"].append(annotation)
      idx += 1

  return coco_annotation




def replace_class_and_average(image: np.ndarray, mask: np.ndarray, class_to_replace=3, background_class=0):
    """
      Replaces a specified class in an image and its mask, averaging background pixels for the replacement.

      Args:
        image (np.ndarray): The input image as a NumPy array.
        mask (np.ndarray): The segmentation mask as a NumPy array (assumed to have the same shape as the image).
        class_to_replace (int, optional): The class value to be replaced in both image and mask. Defaults to 3.
        background_class (int, optional): The class value representing background in the mask. Defaults to 0.

      Returns:
        tuple: A tuple containing two NumPy arrays:
          - modified_image: The image with the specified class replaced using the average background color.
          - modified_mask: The mask with the specified class replaced by the background class value.
    """
    

    # Find background pixels in the image
    background_pixels = image[mask == background_class]
    # Calculate the average of background pixels 
    average_background = np.mean(background_pixels, axis=0)
    
    # Advanced Indexing
    row_idx, col_idx = np.where(mask == class_to_replace)  
    image[row_idx, col_idx] = average_background  

    # Replace in the mask (trivial)
    mask[mask == class_to_replace] = background_class


    # Replace in the image
    image[mask == class_to_replace] = average_background

    

    return image, mask


def squeeze_channels2(images: np.ndarray,
                     labels: np.ndarray,
                     clip: bool = False) -> Tuple[np.ndarray]:
    """
    Squeezes channels in each training image. Unlike AtomAI's version, we do not filter out any image
    This is done because AtomAI filters out patches that don't have all the classes
    
    """

    def squeeze_channels_(label):
        """
        Squeezes multiple channels into a single channel for a single label.

        Args:
          label (np.ndarray): A NumPy array representing a label with multiple channels.

        Returns:
          np.ndarray: A NumPy array representing the same label with channels squeezed into a single channel.
        """
        label_ = np.zeros((1, label.shape[0], label.shape[1]))
        for c in range(label.shape[-1]):
            label_ += label[:, :, c] * c
        return label_

    if labels.shape[-1] == 1:
        return images, labels
    images_valid, labels_valid = [], []
    for label, image in zip(labels, images):
        label = squeeze_channels_(label)
        if clip:
            label[label > labels.shape[-1] - 1] = 0
            labels_valid.append(label)
            images_valid.append(image[None, ...])
        else:
            labels_valid.append(label)
            images_valid.append(image[None, ...])
    return np.concatenate(images_valid), np.concatenate(labels_valid)


def in_list(myarray,mylist):
    return next((True for elem in mylist if array_equal(elem,myarray)),False)

def get_train_test(images,masks,idx_array=[4,5,11]):
    """
      Splits a dataset of images and masks into training and testing sets based on a provided index list.

      Args:
        images (list): A list containing the image data.
        masks (list): A list containing the corresponding segmentation mask data.
          The masks should have the same order as the images in the `images` list.
        idx_array (list[int], optional): A list of indices specifying images to be used for testing. Defaults to [4, 5, 11].

      Returns:
        tuple: A tuple containing four lists:
          - images_train: The training set of images.
          - images_test: The testing set of images.
          - masks_train: The training set of segmentation masks.
          - masks_test: The testing set of segmentation masks.
    """
    images_test=[]
    masks_test=[]
    for idx in idx_array:
        images_test.append(images[idx])
        masks_test.append(masks[idx])
    images_train=[imgs for imgs in images if not in_list(imgs,images_test)]
    masks_train=[msks for msks in masks if not in_list(msks,masks_test)]    
    return images_train, images_test, masks_train, masks_test
        
def one_vs_all_preds(masks,class_id=0):
    """
      Converts a list of binary segmentation masks to a one-vs-all format for a specified class.

      Args:
        masks (list): A list of NumPy arrays representing the binary segmentation masks.
        class_id (int, optional): The class ID to be converted to positive class (1). Defaults to 0.

      Returns:
        list: A list of NumPy arrays representing the one-vs-all encoded masks.
    """
    class_id_masks=[]
    for mask in masks:
        class_id_masks.append((mask == class_id).astype(int))
    return class_id_masks
    
def create_combined_mask(polygons, height,width):
  """
  Creates a single binary mask containing all the polygons.

  Args:
      polygons: A list of polygon contours obtained from cv2.findContours.
      width: Width of the image where the mask will be applied.
      height: Height of the image where the mask will be applied.

  Returns:
      A binary mask (numpy array) with the same dimensions as the image, 
      containing all the polygons.
  """
  # Create a zero-filled mask
  mask = np.zeros((height, width), dtype=np.uint8)
  for poly in polygons:
     # Fill the interiors of all polygons in the mask
     cv2.fillPoly(mask, [poly], 1.0)

  return mask

def create_masks_from_polygon_sets(height: int, width: int, polyset1: list, polyset2: list = None,) -> np.ndarray:
    """
      Creates segmentation masks from two sets of polygons for a given image size.

    Args:
        height (int): The height of the image the masks represent.
        width (int): The width of the image the masks represent.
        polyset1 (list): A list of polygons representing the first object class.
          Each polygon is likely a NumPy array containing vertex coordinates.
        polyset2 (list, optional): A list of polygons representing the second object class (optional). 
          If None, the second mask will be a copy of the first mask. Defaults to None.

    Returns:
        np.ndarray: A NumPy array of shape (height, width, 3) representing the segmentation masks.
          - Axis 0: Background mask (pixels not in either polygon set)
          - Axis 1: Mask for the first object class (defined by polyset1)
          - Axis 2: Mask for the second object class (defined by polyset2, or a copy of mask1 if polyset2 is None)
    """
    mask0 = np.zeros((height, width), dtype=np.uint8)
    mask1 = create_combined_mask(polyset1,height, width)

    if polyset2 is None:
        polyset2=polyset1
        mask2=mask1
    else:
        mask2=create_combined_mask(polyset2,height, width)
    
    mask0=np.logical_not(np.logical_or(mask1,mask2)).astype(np.uint8)
    
    return np.stack([mask0,mask1,mask2],axis=-1)
    
def squeeze_channels_(label):
        """
        Squeezes multiple channel into a single channel for a single label
        """
        label_ = np.zeros((1, label.shape[0], label.shape[1]))
        for c in range(label.shape[-1]):
            label_ += label[:, :, c] * c
        return label_


def seperate_polygons(polyset: list, df: pd.DataFrame) -> tuple:
    """
      Separates polygons into two lists based on class labels in a DataFrame.

      Args:
        polyset (list): A list of polygons, where each polygon is likely a NumPy array representing vertex coordinates.
        df (pd.DataFrame): A pandas DataFrame containing class labels for each polygon.
          It's assumed the DataFrame has a column named 'class' and the indexing aligns with the order of polygons in 'polyset'.

      Returns:
        tuple: A tuple containing two lists:
          - class_0: A list of polygons belonging to class 0 (gas-induced).
          - class_1: A list of polygons belonging to class 1 (lack-of-fusion).
    """
    class_0=[] #gas-induced
    class_1=[] #lack-of-fusion

    for idx in range(len(polyset)):
        poly=polyset[idx]
        if df['class'][idx+1]==1:
            class_1.append(poly)
        else:
            class_0.append(poly)

    return class_0,class_1
            
        
        
def flatten_array(arr):
    """
      Flattens a nested list of arrays into a single list.

      Args:
        arr (list): The list to be flattened, which may contain nested arrays.

      Returns:
        list: A new list containing the elements of the original list in a single dimension.
    """
    flattened_list = []
    for element in arr:
        if isinstance(element, np.ndarray) or isinstance(element,list):
            flattened_list.extend(flatten_array(element))  # Recursive for nested arrays
        else:
            flattened_list.append(element)
    return flattened_list


def get_im(img: np.ndarray, title: str = None, ax=None, cmap_: str = None, overlay=False, mode=2, fonts='xx-small'):
    """
      Displays an image with optional title, colormap, overlay, and legend.

      Args:
        img (np.ndarray): The image data as a NumPy array.
        title (str, optional): The title to display for the image. Defaults to None.
        ax (matplotlib.axes._axes.Axes, optional): A Matplotlib axes object to use for plotting. 
          If None, the current axis is retrieved using plt.gca(). Defaults to None.
        cmap_ (str, optional): The colormap name to use for the image. Defaults to None.
        overlay (bool, optional): If True, enables overlay functionality (not implemented in this code). Defaults to False.
        mode (int, optional): Controls the class labels displayed in the legend. Defaults to 2.
          - 2: Uses 'Gas-Induced' and 'Lack of Fusion' labels.
          - Other values (presumably) use 'Porosity' label.
        fonts (str, optional): Font size for the legend text. Defaults to 'xx-small'.

      Returns:
        None (the function modifies the provided or retrieved axes object for plotting).
    """


    colors = [
        (0, 0, 0, 0),  # Green
        (1.0, 0.0, 0.0,0.6),  # Red
        (0.0, 0.0, 1.0,0.6),  # Blue
        (1.0, 1.0, 0.0,0.6),  # Yellow
    ]
    cmap_mod = mcolors.ListedColormap(colors, name='custom_cmap')
        
    if ax is None:
        ax = plt.gca()  # Get the current axis if none is provided
    ax.imshow(img,cmap=cmap_mod,interpolation='nearest', vmin=0, vmax=4)
    if title is not None:
        ax.set_title(title)
    # plt.colorbar(ticks=range(4))  # Add a colorbar with ticks for each class


    if mode==2:
        class_labels = ['Gas-Induced', 'Lack of Fusion']  # Adjust labels
    else:
        class_labels = ['Porosity']  # Adjust labels
    patches = [mpatches.Patch(color=cmap_mod(i+1), label=label) 
               for i, label in enumerate(class_labels)]
    ax.legend(handles=patches, title='Classes', loc='best',fontsize=fonts,framealpha=1)  # Customize location
    
            
def porosity(image,mode=-1):
    """
      Calculates the percent porosity in a binary image for specific classes or all classes.

      Args:
        image (np.ndarray): A 2D NumPy array representing the binary image.
          - 0 represents background pixels.
          - Positive integer values (1, 2, etc.) represent different classes/pore types (e.g., gas-induced, lack-of-fusion).
        mode (int, optional): The mode to determine which classes to consider as porosity. 
          Defaults to -1 (all classes with values greater than 0).
          - -1: Considers all classes with values greater than 0 as porosity.
          - 1: Considers only class 1 (value 1 in the image) as porosity (gas-induced).
          - 2: Considers only class 2 (value 2 in the image) as porosity (lack-of-fusion).

      Returns:
        float: The percentage of porosity in the image based on the specified mode.
    """
    # Calculate the total number of pixels
    total_pixels = np.prod(image.shape)
    
    if str(mode).lower()=='gas-induced' or mode == 1:
        marked=(image == 1)
    elif str(mode).lower()=='lack-of-fusion' or mode==2:
        marked=(image == 2)
    else:
        marked=(image == 1) | (image == 2)
        
        

    # Calculate the number of pixels that are porosities (classes 1 and 2)
    porosity_pixels = np.count_nonzero(marked)

    # Calculate the percent porosity
    percent_porosity = (porosity_pixels / total_pixels) * 100

    return percent_porosity    
                    
            
        

